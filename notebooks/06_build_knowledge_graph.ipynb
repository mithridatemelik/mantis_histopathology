{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef31783a",
   "metadata": {},
   "source": [
    "# Stage 06 — Build knowledge graph (parquet-first, QA gates, interactive viz)\n",
    "\n",
    "This notebook consumes:\n",
    "- `clusters_semantic.parquet` (Stage 04)\n",
    "- `items_after_agent1.parquet` (Stage 04)\n",
    "- `cluster_links_verified.parquet` (Stage 05)\n",
    "\n",
    "and produces a **Neo4j/RDF-ready knowledge graph**:\n",
    "\n",
    "- `exports/stage_06_kg/kg_nodes.parquet`\n",
    "- `exports/stage_06_kg/kg_edges.parquet`\n",
    "- `exports/stage_06_kg/kg_provenance.parquet`\n",
    "- `exports/stage_06_kg/kg_summary.json`\n",
    "- `exports/stage_06_kg/neo4j_csv/` and optional `kg.ttl`\n",
    "- `exports/stage_06_kg/graph_pyvis.html` (interactive)\n",
    "\n",
    "**New in this refactor**\n",
    "- KG health gates + diagnostics (`kg_quality.py`)\n",
    "- Interactive viz (`pyvis`)\n",
    "- Cluster relationship edges from Agent 2 are added *with evidence attributes*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Colab-first setup ---\n",
    "import os, sys, time\n",
    "from pathlib import Path\n",
    "\n",
    "FORCE_REBUILD = False\n",
    "FAST_MODE = True\n",
    "EDA_LEVEL = \"core\"\n",
    "\n",
    "SHOW_PLOTS = True\n",
    "SAVE_PLOTS = True\n",
    "\n",
    "DRIVE_SEARCH_BASE = \"/content/drive/MyDrive\"\n",
    "\n",
    "def _is_colab() -> bool:\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "if _is_colab():\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "def _resolve_project_root() -> Path:\n",
    "    ev = os.environ.get(\"HISTO_PROJECT_ROOT\")\n",
    "    if ev and Path(ev).exists():\n",
    "        return Path(ev)\n",
    "\n",
    "    base = Path(DRIVE_SEARCH_BASE)\n",
    "    candidates = []\n",
    "    if base.exists():\n",
    "        for p in base.glob(\"**/pipeline_config.yaml\"):\n",
    "            parent = p.parent\n",
    "            if (parent / \"label_taxonomy.yaml\").exists():\n",
    "                candidates.append(parent)\n",
    "    if candidates:\n",
    "        candidates = sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        return candidates[0]\n",
    "\n",
    "    p = Path.cwd()\n",
    "    for _ in range(10):\n",
    "        if (p / \"pipeline_config.yaml\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise FileNotFoundError(\"Could not resolve PROJECT_ROOT. Set HISTO_PROJECT_ROOT env var.\")\n",
    "\n",
    "PROJECT_ROOT = _resolve_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "\n",
    "# Install deps\n",
    "import subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"-r\", str(PROJECT_ROOT / \"requirements.txt\")])\n",
    "\n",
    "import yaml\n",
    "cfg = yaml.safe_load((PROJECT_ROOT / \"pipeline_config.yaml\").read_text())\n",
    "\n",
    "EXPORTS_DIR = PROJECT_ROOT / str(cfg.get(\"paths\", {}).get(\"exports_dir\", \"exports\"))\n",
    "EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAFE_MODE = bool(cfg.get(\"project\", {}).get(\"safe_mode\", True))\n",
    "SEED = int(cfg.get(\"project\", {}).get(\"seed\", 1337))\n",
    "\n",
    "print(\"SAFE_MODE:\", SAFE_MODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d754eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage paths + registries ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from histo_cartography.viz import ensure_dir, save_and_display, register_plot\n",
    "from histo_cartography.artifact_registry import register_artifact, append_stage_manifest\n",
    "from histo_cartography.critic import run_critic, write_critic_report, critic_result_table, critic_issues_table\n",
    "\n",
    "stage_in_clusters = EXPORTS_DIR / \"stage_04_agent1_cleanup\" / \"clusters_semantic.parquet\"\n",
    "stage_in_items = EXPORTS_DIR / \"stage_04_agent1_cleanup\" / \"items_after_agent1.parquet\"\n",
    "\n",
    "# Prefer verified links if available; otherwise fallback to raw cluster_links\n",
    "stage_in_links_verified = EXPORTS_DIR / \"stage_05_agent2_linking\" / \"cluster_links_verified.parquet\"\n",
    "stage_in_links_raw = EXPORTS_DIR / \"stage_05_agent2_linking\" / \"cluster_links.parquet\"\n",
    "\n",
    "assert stage_in_clusters.exists(), f\"missing: {stage_in_clusters}\"\n",
    "assert stage_in_items.exists(), f\"missing: {stage_in_items}\"\n",
    "\n",
    "links_path = stage_in_links_verified if stage_in_links_verified.exists() else stage_in_links_raw\n",
    "assert links_path.exists(), f\"missing links parquet: {links_path}\"\n",
    "\n",
    "stage_dir = EXPORTS_DIR / \"stage_06_kg\"\n",
    "plots_dir = ensure_dir(stage_dir / \"plots\")\n",
    "qa_dir = ensure_dir(stage_dir / \"qa\")\n",
    "neo4j_dir = ensure_dir(stage_dir / \"neo4j_csv\")\n",
    "\n",
    "out_nodes_path = stage_dir / \"kg_nodes.parquet\"\n",
    "out_edges_path = stage_dir / \"kg_edges.parquet\"\n",
    "out_prov_path = stage_dir / \"kg_provenance.parquet\"\n",
    "out_summary_path = stage_dir / \"kg_summary.json\"\n",
    "\n",
    "viz_records = []\n",
    "\n",
    "print(\"links_path:\", links_path)\n",
    "print(\"stage_dir:\", stage_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load upstream artifacts ---\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "clusters_semantic = pd.read_parquet(stage_in_clusters)\n",
    "items_after_agent1 = pd.read_parquet(stage_in_items)\n",
    "cluster_links = pd.read_parquet(links_path)\n",
    "\n",
    "display(clusters_semantic.head(5))\n",
    "display(cluster_links.head(5))\n",
    "\n",
    "print(\"clusters_semantic:\", clusters_semantic.shape)\n",
    "print(\"items_after_agent1:\", items_after_agent1.shape)\n",
    "print(\"cluster_links:\", cluster_links.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4eea27",
   "metadata": {},
   "source": [
    "## PEEP — Preflight health gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de349b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEEP — Critic checks (clusters + items + links)\n",
    "from IPython.display import display\n",
    "\n",
    "crit1 = run_critic(\n",
    "    df=clusters_semantic,\n",
    "    stage=\"stage_06_kg\",\n",
    "    gate=\"peep_clusters_semantic\",\n",
    "    required_cols=[\"cluster_id\",\"cluster_name\",\"cluster_description\"],\n",
    "    id_col=\"cluster_id\",\n",
    "    min_rows=2,\n",
    "    key_nonnull_cols=[\"cluster_id\",\"cluster_name\"],\n",
    ")\n",
    "\n",
    "crit2 = run_critic(\n",
    "    df=items_after_agent1,\n",
    "    stage=\"stage_06_kg\",\n",
    "    gate=\"peep_items_after_agent1\",\n",
    "    required_cols=[\"item_id\",\"cluster_id\",\"source\",\"label\",\"image_path\"],\n",
    "    id_col=\"item_id\",\n",
    "    min_rows=100 if not SAFE_MODE else 10,\n",
    "    key_nonnull_cols=[\"item_id\",\"cluster_id\",\"image_path\"],\n",
    ")\n",
    "\n",
    "# For links, accept either verified or raw schema\n",
    "required_link_cols = [\"src_cluster_id\",\"dst_cluster_id\",\"relationship\"]\n",
    "crit3 = run_critic(\n",
    "    df=cluster_links,\n",
    "    stage=\"stage_06_kg\",\n",
    "    gate=\"peep_cluster_links\",\n",
    "    required_cols=required_link_cols,\n",
    "    id_col=None,\n",
    "    min_rows=0,\n",
    "    key_nonnull_cols=[\"src_cluster_id\",\"dst_cluster_id\",\"relationship\"],\n",
    ")\n",
    "\n",
    "write_critic_report(crit1, qa_dir / \"critic_peep_clusters_semantic.json\")\n",
    "write_critic_report(crit2, qa_dir / \"critic_peep_items_after_agent1.json\")\n",
    "write_critic_report(crit3, qa_dir / \"critic_peep_cluster_links.json\")\n",
    "\n",
    "display(critic_result_table(crit1))\n",
    "display(critic_issues_table(crit1).head(20))\n",
    "display(critic_result_table(crit2))\n",
    "display(critic_issues_table(crit2).head(20))\n",
    "display(critic_result_table(crit3))\n",
    "display(critic_issues_table(crit3).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a35fa",
   "metadata": {},
   "source": [
    "## Stage logic — Build KG tables (idempotent)\n",
    "We build base nodes/edges from items+clusters, then add Agent2 cluster-cluster relationship edges (with evidence attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75444a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build KG ---\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from histo_cartography.exports import save_parquet\n",
    "from histo_cartography.kg import build_kg_tables\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "if out_nodes_path.exists() and out_edges_path.exists() and out_prov_path.exists() and not FORCE_REBUILD:\n",
    "    kg_nodes = pd.read_parquet(out_nodes_path)\n",
    "    kg_edges = pd.read_parquet(out_edges_path)\n",
    "    kg_prov = pd.read_parquet(out_prov_path)\n",
    "    kg_summary = json.loads(out_summary_path.read_text()) if out_summary_path.exists() else {}\n",
    "    print(\"✅ Loaded existing KG artifacts:\", kg_nodes.shape, kg_edges.shape, kg_prov.shape)\n",
    "else:\n",
    "    # Base graph from items + semantic clusters\n",
    "    kg_nodes, kg_edges, kg_prov, kg_summary = build_kg_tables(\n",
    "        items=items_after_agent1,\n",
    "        clusters=clusters_semantic,\n",
    "        include_similarity_edges=False,  # keep KG small; Stage 02/03 already has embedding diagnostics\n",
    "        similarity_top_k=20,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Add cluster-cluster relationship edges (Agent2)\n",
    "    # -----------------------------------------------------------------\n",
    "    # Map cluster_id -> node_id for clusters\n",
    "    cluster_nodes = kg_nodes[kg_nodes[\"node_type\"].astype(str) == \"cluster\"].copy()\n",
    "    cluster_id_to_node = dict(zip(cluster_nodes[\"cluster_id\"].astype(int), cluster_nodes[\"node_id\"].astype(str)))\n",
    "\n",
    "    def _edge_id(src: str, dst: str, rel: str) -> str:\n",
    "        import hashlib\n",
    "        return hashlib.sha256((src + \"|\" + dst + \"|\" + rel).encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "    rel_edges = []\n",
    "    rel_prov = []\n",
    "\n",
    "    for _, r in cluster_links.iterrows():\n",
    "        try:\n",
    "            src_c = int(r[\"src_cluster_id\"])\n",
    "            dst_c = int(r[\"dst_cluster_id\"])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        src = cluster_id_to_node.get(src_c)\n",
    "        dst = cluster_id_to_node.get(dst_c)\n",
    "        if not src or not dst:\n",
    "            continue\n",
    "\n",
    "        rel = str(r.get(\"relationship\", \"related_to\")).upper()  # KG convention\n",
    "        conf = float(r.get(\"confidence\", 0.0) or 0.0)\n",
    "        sim = r.get(\"centroid_similarity\")\n",
    "        sim = float(sim) if sim is not None and sim == sim else None\n",
    "\n",
    "        attrs = {\n",
    "            \"confidence\": conf,\n",
    "            \"centroid_similarity\": sim,\n",
    "            \"needs_more_evidence\": bool(r.get(\"needs_more_evidence\", False)) if \"needs_more_evidence\" in r else None,\n",
    "            \"flags\": r.get(\"flags\"),\n",
    "        }\n",
    "        # store full row (careful: may include nested dicts); convert to JSON-safe\n",
    "        try:\n",
    "            attrs[\"raw_row\"] = {k: (v if isinstance(v, (str,int,float,bool,type(None))) else str(v)) for k, v in r.to_dict().items()}\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        eid = _edge_id(src, dst, rel)\n",
    "        prov_id = \"prov_\" + eid\n",
    "\n",
    "        rel_edges.append(\n",
    "            {\n",
    "                \"edge_id\": eid,\n",
    "                \"src\": src,\n",
    "                \"dst\": dst,\n",
    "                \"rel\": rel,\n",
    "                \"weight\": conf if conf else 1.0,\n",
    "                \"attributes_json\": json.dumps(attrs, ensure_ascii=False),\n",
    "                \"provenance_id\": prov_id,\n",
    "            }\n",
    "        )\n",
    "        rel_prov.append(\n",
    "            {\n",
    "                \"provenance_id\": prov_id,\n",
    "                \"source\": \"stage_05_agent2_linking\",\n",
    "                \"method\": \"agent2_linking\",\n",
    "                \"details_json\": json.dumps({\"links_path\": str(links_path)}, ensure_ascii=False),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    rel_edges_df = pd.DataFrame(rel_edges)\n",
    "    rel_prov_df = pd.DataFrame(rel_prov)\n",
    "\n",
    "    kg_edges = pd.concat([kg_edges, rel_edges_df], ignore_index=True)\n",
    "    kg_prov = pd.concat([kg_prov, rel_prov_df], ignore_index=True)\n",
    "\n",
    "    # Persist\n",
    "    save_parquet(kg_nodes, out_nodes_path)\n",
    "    save_parquet(kg_edges, out_edges_path)\n",
    "    save_parquet(kg_prov, out_prov_path)\n",
    "    out_summary_path.write_text(json.dumps(kg_summary, indent=2))\n",
    "\n",
    "runtime_sec = time.time() - t0\n",
    "print(\"runtime_sec:\", round(runtime_sec, 2))\n",
    "print(\"kg_nodes:\", kg_nodes.shape, \"kg_edges:\", kg_edges.shape, \"kg_prov:\", kg_prov.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aabad08",
   "metadata": {},
   "source": [
    "## CHECKPOINT — KG health gates + diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KG health summary + plots\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "from histo_cartography.kg_quality import build_graph, kg_health_summary, plot_degree_distribution, plot_component_sizes\n",
    "\n",
    "G = build_graph(kg_nodes.rename(columns={\"node_id\":\"node_id\"}), kg_edges)\n",
    "\n",
    "summary = kg_health_summary(kg_nodes.rename(columns={\"node_id\":\"node_id\"}), kg_edges)\n",
    "(qa_dir / \"kg_health_summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "display(summary)\n",
    "\n",
    "# Degree distribution plot\n",
    "fig = plot_degree_distribution(G, title=\"KG degree distribution (in+out)\")\n",
    "out_path = plots_dir / \"kg_degree_distribution.png\"\n",
    "save_and_display(fig, out_path)\n",
    "register_plot(viz_records, stage=\"stage_06_kg\", plot_id=\"kg_degree_distribution\", title=\"KG degree distribution\", path=out_path, tags=[\"core\",\"kg\",\"quality\"], is_core=True)\n",
    "\n",
    "# Component sizes plot\n",
    "fig = plot_component_sizes(G, title=\"KG weakly-connected component sizes\")\n",
    "out_path = plots_dir / \"kg_component_sizes.png\"\n",
    "save_and_display(fig, out_path)\n",
    "register_plot(viz_records, stage=\"stage_06_kg\", plot_id=\"kg_component_sizes\", title=\"KG component sizes\", path=out_path, tags=[\"core\",\"kg\",\"quality\"], is_core=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e08258",
   "metadata": {},
   "source": [
    "## Exports — Neo4j CSV + RDF Turtle (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a664488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Neo4j CSV and optional RDF\n",
    "import pandas as pd\n",
    "\n",
    "from histo_cartography.exports import export_neo4j_csv, export_rdf_turtle\n",
    "\n",
    "# Convert nodes -> entities expected by export helpers\n",
    "entities = kg_nodes.rename(columns={\"node_id\":\"entity_id\",\"node_type\":\"entity_type\"}).copy()\n",
    "entities[\"entity_type\"] = entities[\"entity_type\"].astype(str).str.upper()\n",
    "entities[\"description\"] = entities.get(\"description\", \"\").astype(str)\n",
    "\n",
    "neo_paths = export_neo4j_csv(entities=entities, edges=kg_edges, out_dir=neo4j_dir)\n",
    "print(\"✅ Neo4j export:\", neo_paths)\n",
    "\n",
    "ttl_path = stage_dir / \"kg.ttl\"\n",
    "ttl = export_rdf_turtle(entities=entities, edges=kg_edges, out_path=ttl_path)\n",
    "if ttl:\n",
    "    print(\"✅ RDF export:\", ttl)\n",
    "else:\n",
    "    print(\"⚠️ rdflib not installed; RDF export skipped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75150898",
   "metadata": {},
   "source": [
    "## Interactive graph view (PyVis)\n",
    "This is a lightweight HTML you can open/download from Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyVis interactive HTML (small subgraph to keep it fast)\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from pyvis.network import Network  # type: ignore\n",
    "except Exception as e:\n",
    "    print(\"⚠️ pyvis not available:\", e)\n",
    "    Network = None\n",
    "\n",
    "# Build a small subgraph: clusters + their direct relationship edges only\n",
    "cluster_nodes = kg_nodes[kg_nodes[\"node_type\"].astype(str) == \"cluster\"].copy()\n",
    "cluster_ids = set(cluster_nodes[\"node_id\"].astype(str).tolist())\n",
    "\n",
    "rel_edges = kg_edges[kg_edges[\"src\"].astype(str).isin(cluster_ids) & kg_edges[\"dst\"].astype(str).isin(cluster_ids)].copy()\n",
    "print(\"cluster_nodes:\", cluster_nodes.shape, \"cluster_rel_edges:\", rel_edges.shape)\n",
    "\n",
    "if Network is not None:\n",
    "    net = Network(height=\"700px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\", directed=True)\n",
    "\n",
    "    # Add nodes\n",
    "    for _, r in cluster_nodes.iterrows():\n",
    "        nid = str(r[\"node_id\"])\n",
    "        label = str(r.get(\"name\", \"\"))[:60]\n",
    "        title = str(r.get(\"description\", \"\"))[:300]\n",
    "        net.add_node(nid, label=label, title=title)\n",
    "\n",
    "    # Add edges\n",
    "    for _, r in rel_edges.iterrows():\n",
    "        net.add_edge(str(r[\"src\"]), str(r[\"dst\"]), title=str(r.get(\"rel\",\"\")), value=float(r.get(\"weight\",1.0)))\n",
    "\n",
    "    out_html = stage_dir / \"graph_pyvis.html\"\n",
    "    net.write_html(str(out_html))\n",
    "    print(\"✅ wrote:\", out_html)\n",
    "else:\n",
    "    print(\"skipping pyvis export\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626fac3",
   "metadata": {},
   "source": [
    "## Register artifacts + viz index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register stage artifacts + viz index\n",
    "schema_version = str(cfg.get(\"project\", {}).get(\"schema_version\", \"0.1.0\"))\n",
    "\n",
    "# Critic on nodes/edges\n",
    "crit_nodes = run_critic(\n",
    "    df=kg_nodes,\n",
    "    stage=\"stage_06_kg\",\n",
    "    gate=\"checkpoint_kg_nodes\",\n",
    "    required_cols=[\"node_id\",\"node_type\",\"name\"],\n",
    "    id_col=\"node_id\",\n",
    "    min_rows=10,\n",
    "    key_nonnull_cols=[\"node_id\",\"node_type\"],\n",
    ")\n",
    "crit_edges = run_critic(\n",
    "    df=kg_edges,\n",
    "    stage=\"stage_06_kg\",\n",
    "    gate=\"checkpoint_kg_edges\",\n",
    "    required_cols=[\"edge_id\",\"src\",\"dst\",\"rel\"],\n",
    "    id_col=\"edge_id\",\n",
    "    min_rows=10,\n",
    "    key_nonnull_cols=[\"edge_id\",\"src\",\"dst\",\"rel\"],\n",
    ")\n",
    "\n",
    "write_critic_report(crit_nodes, qa_dir / \"critic_checkpoint_kg_nodes.json\")\n",
    "write_critic_report(crit_edges, qa_dir / \"critic_checkpoint_kg_edges.json\")\n",
    "\n",
    "register_artifact(\n",
    "    project_root=PROJECT_ROOT,\n",
    "    stage=\"stage_06_kg\",\n",
    "    artifact=\"kg_nodes\",\n",
    "    path=out_nodes_path,\n",
    "    schema_version=schema_version,\n",
    "    inputs=[stage_in_clusters, stage_in_items, links_path],\n",
    "    df=kg_nodes,\n",
    "    warnings_count=int(crit_nodes.warnings_count + crit_edges.warnings_count),\n",
    "    fails_count=int(crit_nodes.fails_count + crit_edges.fails_count),\n",
    "    runtime_sec=float(runtime_sec),\n",
    "    notes=\"KG nodes table\",\n",
    ")\n",
    "register_artifact(\n",
    "    project_root=PROJECT_ROOT,\n",
    "    stage=\"stage_06_kg\",\n",
    "    artifact=\"kg_edges\",\n",
    "    path=out_edges_path,\n",
    "    schema_version=schema_version,\n",
    "    inputs=[stage_in_clusters, stage_in_items, links_path],\n",
    "    df=kg_edges,\n",
    "    warnings_count=int(crit_nodes.warnings_count + crit_edges.warnings_count),\n",
    "    fails_count=int(crit_nodes.fails_count + crit_edges.fails_count),\n",
    "    runtime_sec=float(runtime_sec),\n",
    "    notes=\"KG edges table\",\n",
    ")\n",
    "register_artifact(\n",
    "    project_root=PROJECT_ROOT,\n",
    "    stage=\"stage_06_kg\",\n",
    "    artifact=\"kg_provenance\",\n",
    "    path=out_prov_path,\n",
    "    schema_version=schema_version,\n",
    "    inputs=[stage_in_clusters, stage_in_items, links_path],\n",
    "    df=kg_prov,\n",
    "    warnings_count=int(crit_nodes.warnings_count + crit_edges.warnings_count),\n",
    "    fails_count=int(crit_nodes.fails_count + crit_edges.fails_count),\n",
    "    runtime_sec=float(runtime_sec),\n",
    "    notes=\"KG provenance table\",\n",
    ")\n",
    "\n",
    "append_stage_manifest(\n",
    "    project_root=PROJECT_ROOT,\n",
    "    stage=\"stage_06_kg\",\n",
    "    inputs=[stage_in_clusters, stage_in_items, links_path],\n",
    "    outputs=[out_nodes_path, out_edges_path, out_prov_path],\n",
    "    schema_version=schema_version,\n",
    "    warnings_count=int(crit_nodes.warnings_count + crit_edges.warnings_count),\n",
    "    fails_count=int(crit_nodes.fails_count + crit_edges.fails_count),\n",
    "    runtime_sec=float(runtime_sec),\n",
    "    notes=\"stage 06 run summary\",\n",
    ")\n",
    "\n",
    "# viz index\n",
    "from histo_cartography.viz import write_viz_index, viz_records_to_df\n",
    "from IPython.display import display\n",
    "\n",
    "viz_index_path = stage_dir / \"viz_index.parquet\"\n",
    "write_viz_index(viz_records, out_parquet=viz_index_path, out_csv=stage_dir / \"viz_index.csv\")\n",
    "display(viz_records_to_df(viz_records).head(200))\n",
    "print(\"✅ wrote viz_index:\", viz_index_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3aa9af",
   "metadata": {},
   "source": [
    "## Next actions\n",
    "- Load `neo4j_csv/` into Neo4j or use `graph_pyvis.html` for a quick interactive check.\n",
    "- If KG has many components/orphans: revisit Stage 05 relationships (add more links or lower MIN_SIM cautiously)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
